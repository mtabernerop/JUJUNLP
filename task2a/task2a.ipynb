{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "riwHXQ137m2C"
      },
      "source": [
        "# HUHU@IberLEF2023 Task 2a (Multi-label Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyawhEUEYqGe"
      },
      "source": [
        "Task: https://sites.google.com/view/huhuatiberlef23/huhu\n",
        "\n",
        "This notebook contains the code to fine-tune several pre-trained transformers for the task of hurtful humour detection (multi-label classification).\n",
        "\n",
        "In particular, the models are:\n",
        "\n",
        "* BERT Multilingual: ``bert-base-multilingual-cased`` and ``bert-base-multilingual-uncased``\n",
        "* RoBERTa: ``roberta-base``\n",
        "* BETO: ``dccuchile/bert-base-spanish-wwm-cased`` and ``dccuchile/bert-base-spanish-wwm-uncased``\n",
        "* DistilBERT Multilingual: ``distilbert-base-multilingual-cased``\n",
        "\n",
        "To take advantage of these transformer models, different ensembles are configured resulting from all their possible combinations.\n",
        "\n",
        "Experiments show that combining the prediction capabilities of these models allow to achieve better results than when used independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBcJtF32aekx"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiX8n7eE7m2G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability on Google Colab\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvcO44JAwkG"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install simpletransformers\n",
        "!pip install datasets\n",
        "!pip install ipywidgets\n",
        "!pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErLOSYoXQAk9"
      },
      "outputs": [],
      "source": [
        "# Define global variables\n",
        "\n",
        "SEED = 42 # allow for experiments' reproductibility\n",
        "WEIGHTED = True # use weighted ensemble (in favour of models with higher F1-score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEN2_uO77zQF"
      },
      "source": [
        "# Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CeYjwOHT2Vn"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# Notebook login via HF's token\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxzDl1jl8NQg"
      },
      "outputs": [],
      "source": [
        "from datasets import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Avoid warnings\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Load training, validation and test splits\n",
        "train = pd.DataFrame(load_dataset(\"huhu2023/multilabel-huhu2023\", split=\"train\"))\n",
        "val = pd.DataFrame(load_dataset(\"huhu2023/multilabel-huhu2023\", split=\"validation\"))\n",
        "test = pd.DataFrame(load_dataset(\"huhu2023/multilabel-huhu2023\", split=\"test\"))\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MAWoisl9rcl"
      },
      "outputs": [],
      "source": [
        "# Define label encoding\n",
        "labels = [\"prejudice_woman\", \"prejudice_lgbtiq\", \"prejudice_inmigrant_race\", \"gordofobia\"]\n",
        "\n",
        "# Function to rename fields and drop unnecessary ones\n",
        "def get_text_and_label(df):\n",
        "  return df.rename(columns={\"tweet\": \"text\"})[[\"text\"] + labels]\n",
        "\n",
        "# Get treated dataframe for training, validation and test splits\n",
        "train = get_text_and_label(train)\n",
        "val = get_text_and_label(val)\n",
        "test = get_text_and_label(test)\n",
        "\n",
        "print(f\"Dataset size: <{len(train.index)}:{len(val.index)}:{len(test.index)}>\")\n",
        "train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NCIVhBmTWiW"
      },
      "source": [
        "The ```simpletransformers``` Python library that will be used below requires the data to be presented in a specific form.\n",
        "\n",
        "The following cell adapts each split to contain only two columns: ```text``` and ```labels```, where the latter is an array equal in size to the number of labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-Q1iOiJg4Zt"
      },
      "outputs": [],
      "source": [
        "def generate_multilabel_df(df):\n",
        "  # Initialize new \"labels\" column to arrays of <len(labels)> 0s\n",
        "  df[\"labels\"] = np.empty((len(df), len(labels))).tolist()\n",
        "\n",
        "  # Traverse all rows in split dataframe\n",
        "  for index, row in df.iterrows():\n",
        "    # Get labels of current instance\n",
        "    row_labels = [row[label] for label in labels]\n",
        "\n",
        "    # Copy the instance labels to the new column\n",
        "    for i in range(len(labels)):\n",
        "      df.loc[index, \"labels\"][i] = row_labels[i]\n",
        "\n",
        "  # Preserve only the \"text\" and \"labels\" fields\n",
        "  return df[[\"text\", \"labels\"]]\n",
        "\n",
        "train = generate_multilabel_df(train)\n",
        "val = generate_multilabel_df(val)\n",
        "test = generate_multilabel_df(test)\n",
        "\n",
        "train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo9HkmprFv1i"
      },
      "source": [
        "# Create output directory\n",
        "\n",
        "The output directory structure is defined. Each of the transformer models will be saved, along with their results. Metrics regarding the performance of the ensembles will be also collected for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6VMWi6WFvmC"
      },
      "outputs": [],
      "source": [
        "# Load and mount the Drive helper\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WT8gTpNFyiU"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Define unique path for current experiment\n",
        "PATH = \"/path/to/task2a/outputs/{}/\".format((datetime.now() + timedelta(hours=2)).strftime(\"%d-%m-%Y-%H-%M\"))\n",
        "print(\"Current working dir:\", PATH)\n",
        "\n",
        "# Create directory\n",
        "os.mkdir(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_7xlRP57m2I"
      },
      "source": [
        "# Models' definition\n",
        "\n",
        "In this section, the different transformers that will be evaluated are gathered. For this purpose, the implementation mainly relies in the ``simpletransformers`` Python library, which allows to train and test transformers within few steps.\n",
        "\n",
        "For further information: https://simpletransformers.ai/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDYhWuuqGUXI"
      },
      "outputs": [],
      "source": [
        "# Define transformers' initialization dictionary \n",
        "models = {\n",
        "    \"mbert-cased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"bert-base-multilingual-cased\"\n",
        "    },\n",
        "    \"mbert-uncased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"bert-base-multilingual-uncased\"\n",
        "    },\n",
        "    \"roberta\": {\n",
        "        \"model_type\": \"roberta\",\n",
        "        \"model_name\": \"roberta-base\"\n",
        "    },\n",
        "    \"beto-cased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\"\n",
        "    },\n",
        "    \"beto-uncased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "    },\n",
        "    \"distilbert-multi\": {\n",
        "        \"model_type\": \"distilbert\",\n",
        "        \"model_name\": \"distilbert-base-multilingual-cased\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmBFdLAuAr8n"
      },
      "outputs": [],
      "source": [
        "# Import pre-trained simpletransformers models for classification\n",
        "from simpletransformers.classification import MultiLabelClassificationModel, MultiLabelClassificationArgs\n",
        "\n",
        "# Define a dictionary where each key matches its corresponding transformer\n",
        "# All transformers share the same classification arguments\n",
        "for model, fields in models.items():\n",
        "\n",
        "  # Define models' classification arguments\n",
        "  model_args = MultiLabelClassificationArgs(\n",
        "      overwrite_output_dir= True,\n",
        "      eval_batch_size=8,\n",
        "      num_train_epochs=5,\n",
        "      learning_rate = 4e-05,\n",
        "      optimizer=\"AdamW\",\n",
        "      manual_seed=SEED,\n",
        "      use_early_stopping=True,\n",
        "      save_model_every_epoch=False\n",
        "  )\n",
        "\n",
        "  model_args.output_dir = os.path.join(PATH, model)\n",
        "  # os.mkdir(model_args.output_dir)\n",
        "  models[model] = MultiLabelClassificationModel(fields[\"model_type\"], fields[\"model_name\"],\n",
        "                                      args=model_args, num_labels=len(labels), use_cuda=use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqfNi4v2-wqv"
      },
      "source": [
        "# Training\n",
        "\n",
        "Each of the aforementioned models is trained separatedly with the entire training set.\n",
        "\n",
        "This training is directly performed in the previously defined dictionary for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgDvPyOX2k8h"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Define RMSE function\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return sqrt(mean_squared_error(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf82AZdd_F8k"
      },
      "outputs": [],
      "source": [
        "# Train all models with training set instances\n",
        "for model_name, model in models.items():\n",
        "  model.train_model(train, loss_fct=root_mean_squared_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff815Ug5igF3"
      },
      "source": [
        "# Ensembles' definition\n",
        "\n",
        "The ensembles of transformers that can be defined with the previously trained models are created.\n",
        "\n",
        "A dictionary is create for convenience, univocally identifying each ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q44p52krifIb"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from itertools import combinations\n",
        "\n",
        "# Define a list containing the lists of models of each ensemble\n",
        "models_names = list(models.keys())\n",
        "ensembles_list = list()\n",
        "\n",
        "for i in range(1, len(models_names) + 1):\n",
        "    ensembles_list += list(combinations(models_names, i))\n",
        "ensembles_list = [list(ensemble) for ensemble in ensembles_list]\n",
        "\n",
        "# Define a dictionary with the ensembles\n",
        "ensembles = {}\n",
        "for i in range(len(ensembles_list)):\n",
        "  ensembles[\"ensemble{:02d}\".format(i)] = {}\n",
        "  ensembles[\"ensemble{:02d}\".format(i)][\"models\"] = ensembles_list[i]\n",
        "  ensembles[\"ensemble{:02d}\".format(i)][\"metrics\"] = {}\n",
        "ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phs1siYG7m2J"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Firstly, each transformer is individually evaluated using the validation split. Subsequently, the main evaluation metrics (accuracy, F1-score, precision and recall) are stored.\n",
        "\n",
        "Secondly, the predictions of each ensemble for the validation set instances are derived. After calculating their metrics, it is possible to determine which ensemble obtained the best F1-Score. This will be the final ensemble used for the test dataset.\n",
        "\n",
        "Regarding the ensembles' predictions, these are obtained through a hard voting system: after computing the output that each of the ensemble's models produces for a given instance, the most-voted class turns out to be the ensemble result.\n",
        "\n",
        "The voting system can be non-weighted or weighted. In the latter, the prediction of each individual transformer is weighted according to their normalized F1-score, thus providing a greater importance to the best model without disregarding the outputs of the other transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btUPwIXBAzcT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
        "\n",
        "# Function which computes the evaluation metrics given two lists of true and\n",
        "# predicted labels\n",
        "def compute_metrics(y_true, y_pred):\n",
        "  macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "  return {\n",
        "      'accuracy': round(acc, 5),\n",
        "      'macro_f1': round(macro_f1, 5),\n",
        "      'macro_precision': round(macro_precision, 5),\n",
        "      'macro_recall': round(macro_recall, 5),\n",
        "      'weighted_f1': round(weighted_f1, 5)\n",
        "  }\n",
        "\n",
        "# Transformers' evaluation under the validation set\n",
        "model_evaluation = {}\n",
        "for model_name in models:\n",
        "  model_evaluation[model_name] = {}\n",
        "  # Storing the prediction outputs\n",
        "  result, model_outputs, wrong_predictions = models[model_name].eval_model(val, metric=root_mean_squared_error)\n",
        "  model_evaluation[model_name][\"result\"] = result                                                                    # Result\n",
        "  model_evaluation[model_name][\"val_model_outputs\"] = model_outputs                                                  # Raw model ouputs \n",
        "  model_evaluation[model_name][\"val_predictions\"] = list ()\n",
        "  for output in model_outputs:\n",
        "    model_evaluation[model_name][\"val_predictions\"].append([0 if lab_output <= 0.5 else 1 for lab_output in output])  # Class prediction\n",
        "  model_evaluation[model_name][\"val_wrong_predictions\"] = wrong_predictions                                          # Wrongly-predicted instances\n",
        "  \n",
        "  # Storing the metrics\n",
        "  model_evaluation[model_name][\"metrics\"] = compute_metrics(val.get(\"labels\").tolist(), model_evaluation[model_name].get(\"val_predictions\"))\n",
        "  print(f\"{model_name}\\t\", model_evaluation[model_name].get(\"metrics\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr7NHxjYo5xV"
      },
      "source": [
        "The ``vote`` function determines the ensembler prediction based on the outcomes of its transformers. Its arguments are:\n",
        "1.   ``predictions``: list of transformers' (raw) outputs\n",
        "2.   ``weighted``: bool that determines if a weighted voting system must be used\n",
        "3.   ``weights``: list of weights (normalized weighted F1-scores) \n",
        "\n",
        "The ``predict_ensemble``function calculates the predictions of each ensemble for a given dataset split (``dataset_name``, ``dataset``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "echFpwerAO_n"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Function which determines the ensembler prediction based on its\n",
        "# transformers' predictions. A weighted voting system may be used\n",
        "def vote(predictions, weighted=False, weights=None):\n",
        "  votes = list()\n",
        "  # Get the vote for each label individually\n",
        "  for i in range(len(labels)):\n",
        "    # Get the models' predictions for the current label\n",
        "    curr_label_preds = [preds[i] for preds in predictions]\n",
        "    # Calculate and append the binary for the current label\n",
        "    voting = sum(curr_label_preds * weights) if weighted else sum(curr_label_preds)/len(curr_label_preds)\n",
        "    votes.append(0 if voting <= 0.5 else 1)\n",
        "  return votes\n",
        "\n",
        "# ensemble metrics\n",
        "ensemble_evaluation = {}\n",
        "\n",
        "# Function to predict the label of the instances in a dataset split (validation\n",
        "# (\"val\") or test (\"test\")) for each ensemble\n",
        "def predict_ensemble(ensemble_name, dataset_name, dataset, weighted=False):\n",
        "  ensemble_evaluation[ensemble_name][f\"{dataset_name}_predictions\"] = list()\n",
        "  # Traverse each dataset instance\n",
        "  for i in range(len(dataset.index)):\n",
        "    predictions = list()\n",
        "    ensemble_models = ensembles[ensemble_name].get(\"models\")\n",
        "    # Get the raw output of each model in the ensemble for the instance at hand\n",
        "    for model_name in ensemble_models:\n",
        "      curr_model_outputs = model_evaluation[model_name].get(f\"{dataset_name}_model_outputs\")\n",
        "      predictions.append(curr_model_outputs[i])\n",
        "\n",
        "    # Define the list of weights if a weighted voting system must be used\n",
        "    weights = list()\n",
        "    if weighted:\n",
        "      # The weights' list is obtained by normalizing the weighted F1-scores of\n",
        "      # the models in the ensemble\n",
        "      f1_scores_list = [model_evaluation[model_name][\"metrics\"].get(\"weighted_f1\")\n",
        "                        for model_name in ensembles[ensemble_name].get(\"models\")]\n",
        "      weights = normalize([f1_scores_list], norm=\"l1\")[0]\n",
        "\n",
        "    # Append the predicted label to the predictions of the ensemble\n",
        "    ensemble_pred = vote(predictions, weighted, weights)\n",
        "    ensemble_evaluation[ensemble_name][f\"{dataset_name}_predictions\"].append(ensemble_pred)\n",
        "\n",
        "# Ensembles' evaluation under the validation set\n",
        "for ensemble_name in ensembles:\n",
        "  ensemble_evaluation[ensemble_name] = {}\n",
        "  ensemble_evaluation[ensemble_name][\"val_predictions\"] = list()\n",
        "  predict_ensemble(ensemble_name, \"val\", val, weighted=WEIGHTED)\n",
        "  ensembles[ensemble_name][\"metrics\"] = compute_metrics(val.get(\"labels\").tolist(), ensemble_evaluation[ensemble_name].get(\"val_predictions\"))\n",
        "  print(f\"{ensemble_name}\\t\", ensembles[ensemble_name].get(\"metrics\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EJCzIq4J9x8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save ensembles to JSON file\n",
        "with open(os.path.join(PATH, 'ensembles.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(ensembles, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ2VBWpGt1Ob"
      },
      "source": [
        "# Selecting the best ensemble\n",
        "\n",
        "Once the predicted labels for each validation instance are calculated for each ensemble, their metrics can be computed. Given that it is a binary classification task, the best ensemble will be that with a maximum F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOuzbgJax6oZ"
      },
      "outputs": [],
      "source": [
        "# Defining a dictionary with the weighted F1-score of each ensemble\n",
        "f1_scores = {ensemble_name: ensembles[ensemble_name][\"metrics\"].get(\"weighted_f1\") for ensemble_name in ensemble_evaluation}\n",
        "# Selecting the best ensemble\n",
        "best_ensemble_name = max(f1_scores, key=f1_scores.get)\n",
        "best_ensemble = {\"name\": best_ensemble_name,\n",
        "                 \"models\": ensembles[best_ensemble_name].get(\"models\"),\n",
        "                 \"metrics\": ensembles[best_ensemble_name].get(\"metrics\")\n",
        "                 }\n",
        "\n",
        "best_ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyNjhkTUF3eU"
      },
      "source": [
        "# Predictions on test set\n",
        "\n",
        "Finally, the ensemble which obtained a higher F1-score can be used to predict the label of each test instance.\n",
        "\n",
        "Further, these results will be used to portray some evaluation plots, including the Confusion Matrix on the positive class (\"humour\") and the ROC curve. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OPMaerZF5IA"
      },
      "outputs": [],
      "source": [
        "# Predicting the label of the test set's instances with each individual\n",
        "# transformer that composes the best ensemble\n",
        "for model_name in models:\n",
        "  model_predictions, model_raw_outputs = models.get(model_name).predict(test[\"text\"].tolist())\n",
        "  model_evaluation[model_name][\"test_model_outputs\"] = model_raw_outputs\n",
        "  model_evaluation[model_name][\"test_predictions\"] = list ()\n",
        "  for output in model_raw_outputs:\n",
        "    model_evaluation[model_name][\"test_predictions\"].append([0 if lab_output <= 0.5 else 1 for lab_output in output])\n",
        "\n",
        "# Calculating the test predictions of the best ensemble\n",
        "predict_ensemble(best_ensemble.get(\"name\"), \"test\", test, weighted=WEIGHTED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Wn8BZvEKZK4"
      },
      "outputs": [],
      "source": [
        "# Dump individual transformers' results\n",
        "for model_name, evaluation in model_evaluation.items():\n",
        "\n",
        "  curr_model = model_evaluation.get(model_name)\n",
        "\n",
        "  # Converting ndarrays to lists\n",
        "  curr_model[\"val_model_outputs\"] = [list(output) for output in curr_model.get(\"val_model_outputs\")]\n",
        "  curr_model[\"test_model_outputs\"] = [list(output) for output in curr_model.get(\"test_model_outputs\")]\n",
        "  curr_model[\"val_predictions\"] = list(curr_model.get(\"val_predictions\"))\n",
        "  curr_model[\"test_predictions\"] = list(curr_model.get(\"test_predictions\"))\n",
        "  \n",
        "  # Adapting validation wrong predictions (if any)\n",
        "  if curr_model.get(\"val_wrong_predictions\"):\n",
        "    curr_model[\"val_wrong_predictions_list\"] = curr_model.get(\"val_wrong_predictions\")\n",
        "    curr_model[\"val_wrong_predictions\"] = {}\n",
        "    for pred in curr_model.get(\"val_wrong_predictions_list\"):\n",
        "      curr_model[\"val_wrong_predictions\"][pred.guid] = {\n",
        "          \"text_a\": pred.text_a,\n",
        "          \"text_b\": pred.text_b,\n",
        "          \"label\": pred.label\n",
        "      }\n",
        "    del curr_model[\"val_wrong_predictions_list\"]\n",
        "\n",
        "  with open(os.path.join(PATH, f'{model_name}/model-evaluation.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(curr_model, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qldy3WivKnmh"
      },
      "outputs": [],
      "source": [
        "# Complete fields of best ensemble dictionary\n",
        "best_ensemble[\"val_predictions\"] = ensemble_evaluation[best_ensemble.get(\"name\")].get(\"val_predictions\")\n",
        "best_ensemble[\"test_predictions\"] = ensemble_evaluation[best_ensemble.get(\"name\")].get(\"test_predictions\")\n",
        "\n",
        "# Save best ensemble to JSON file\n",
        "with open(os.path.join(PATH, 'best-ensemble.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(best_ensemble, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOQ10-8OGE8x"
      },
      "outputs": [],
      "source": [
        "# Creating a new column of predicted labels in the test dataframe\n",
        "test[\"predicted_labels\"] = ensemble_evaluation[best_ensemble.get(\"name\")].get(\"test_predictions\")\n",
        "test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OovdWdo1KtUJ"
      },
      "outputs": [],
      "source": [
        "# Dump test predictions\n",
        "test.to_csv(os.path.join(PATH, \"test-predictions.csv\"), index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwdzIe6Pv3Ke"
      },
      "source": [
        "## Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQLzXbt8plBt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Classification report\n",
        "cr = classification_report(y_true=test[\"labels\"].tolist(), y_pred=test[\"predicted_labels\"].tolist(), target_names=labels)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhqqEOMJLUv3"
      },
      "outputs": [],
      "source": [
        "# Dump classification report\n",
        "with open(os.path.join(PATH, \"classification-report.txt\"), \"w\") as f:\n",
        "  f.write(cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVeIDMGBv4-v"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He8DdKPh5JOF"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig_cm, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in range(len(labels)):\n",
        "  y_true = [label[i] for label in test[\"labels\"].tolist()]\n",
        "  y_pred = [label[i] for label in test[\"predicted_labels\"].tolist()]\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred), display_labels=[f\"NOT-Class{i+1}\", f\"YES-Class{i+1}\"])\n",
        "  disp.plot(ax=axes[i], values_format='.4g')\n",
        "  disp.ax_.set_title(\"Class {}\".format(labels[i]), fontsize=15)\n",
        "  if i<2:\n",
        "    disp.ax_.set_xlabel('')\n",
        "  else:\n",
        "    disp.ax_.set_xlabel('Predicted Values', fontsize=15)\n",
        "  if i%2!=0:\n",
        "    disp.ax_.set_ylabel('')\n",
        "  else:\n",
        "    disp.ax_.set_ylabel('True Values', fontsize=15)\n",
        "  disp.im_.colorbar.remove()\n",
        "  disp.ax_.set_xticklabels(disp.ax_.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3, hspace=0.1)\n",
        "fig_cm.colorbar(disp.im_, ax=axes, fraction=0.04625, pad=0.04)\n",
        "plt.suptitle(\"Confusion Matrices\", fontsize=20, y=0.925)\n",
        "\n",
        "# Save CM\n",
        "fig_cm.savefig(os.path.join(PATH, \"CONFUSION MATRIX.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVghIyzv-U6"
      },
      "source": [
        "## ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTOFdCbAFhln"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "\n",
        "fig_roc = plt.figure(figsize = (10.8, 10.8))\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "for i in range(len(labels)):\n",
        "  y_true = [label[i] for label in test[\"labels\"].tolist()]\n",
        "  y_pred = [label[i] for label in test[\"predicted_labels\"].tolist()]\n",
        "  fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
        "  plt.plot(fpr, tpr, label=\"ROC curve of class {} (AUC = {:0.2f})\".format(labels[i], auc(fpr, tpr)))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\", color = \"darkblue\", linestyle = \"--\", label = \"Random Classifier (AUC = 0.5)\") # AUC: Area Under Curve\n",
        "plt.axis(\"square\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\", fontsize = 15)\n",
        "plt.ylabel(\"True Positive Rate (TPR)\", fontsize = 15)\n",
        "plt.title(\"ROC curve\", fontsize = 20)\n",
        "plt.tick_params(axis = \"y\",direction = \"in\")\n",
        "plt.tick_params(axis = \"x\",direction = \"in\")\n",
        "plt.legend()\n",
        "\n",
        "# Save ROC curve\n",
        "fig_roc.savefig(os.path.join(PATH, \"ROC.png\"))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
