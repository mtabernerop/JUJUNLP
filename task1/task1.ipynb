{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riwHXQ137m2C"
      },
      "source": [
        "# HUHU@IberLEF2023 Task 1 (Binary Classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyawhEUEYqGe"
      },
      "source": [
        "Task: https://sites.google.com/view/huhuatiberlef23/huhu\n",
        "\n",
        "This notebook contains the code to fine-tune several pre-trained transformers for the task of hurtful humour detection (binary classification).\n",
        "\n",
        "In particular, the models are:\n",
        "\n",
        "* BERT Multilingual: ``bert-base-multilingual-cased`` and ``bert-base-multilingual-uncased``\n",
        "* RoBERTa: ``roberta-base``\n",
        "* BETO: ``dccuchile/bert-base-spanish-wwm-cased`` and ``dccuchile/bert-base-spanish-wwm-uncased``\n",
        "* DistilBERT Multilingual: ``distilbert-base-multilingual-cased``\n",
        "\n",
        "To take advantage of these transformer models, different ensembles are configured resulting from all their possible combinations.\n",
        "\n",
        "Experiments show that combining the prediction capabilities of these models allow to achieve better results than when used independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBcJtF32aekx"
      },
      "source": [
        "# Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiX8n7eE7m2G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability on Google Colab\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvcO44JAwkG"
      },
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install simpletransformers\n",
        "!pip install datasets\n",
        "!pip install ipywidgets\n",
        "!pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErLOSYoXQAk9"
      },
      "outputs": [],
      "source": [
        "# Define global variables\n",
        "\n",
        "SEED = 42 # allow for experiments' reproductibility\n",
        "WEIGHTED = True # use weighted ensemble (in favour of models with higher F1-score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEN2_uO77zQF"
      },
      "source": [
        "# Dataset load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CeYjwOHT2Vn"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# Notebook login via HF's token\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxzDl1jl8NQg"
      },
      "outputs": [],
      "source": [
        "from datasets import *\n",
        "import pandas as pd\n",
        "\n",
        "# Avoid warnings\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Load training, validation and test splits\n",
        "train = pd.DataFrame(load_dataset(\"huhu2023/bin-huhu2023\", split=\"train\"))\n",
        "val = pd.DataFrame(load_dataset(\"huhu2023/bin-huhu2023\", split=\"validation\"))\n",
        "test = pd.DataFrame(load_dataset(\"huhu2023/bin-huhu2023\", split=\"test\"))\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MAWoisl9rcl"
      },
      "outputs": [],
      "source": [
        "# Function to rename fields and drop unnecessary ones\n",
        "def get_text_and_label(df, original_dataset=True):\n",
        "  text_tag = \"tweet\" if original_dataset else \"text\"\n",
        "  label_tag = \"humor\" if original_dataset else \"is_humor\"\n",
        "  return df.rename(columns={text_tag: \"text\", label_tag: \"label\"})[[\"text\", \"label\"]]\n",
        "\n",
        "# Get treated dataframe for training, validation and test splits\n",
        "train = get_text_and_label(train)\n",
        "val = get_text_and_label(val)\n",
        "test = get_text_and_label(test)\n",
        "\n",
        "print(f\"Dataset size: <{len(train.index)}:{len(val.index)}:{len(test.index)}>\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIqouPVKTVzM"
      },
      "source": [
        "# Create output directory\n",
        "\n",
        "The output directory structure is defined. Each of the transformer models will be saved, along with their results. Metrics regarding the performance of the ensembles will be also collected for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6TsRVocWi-J"
      },
      "outputs": [],
      "source": [
        "# Load and mount the Drive helper\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G31dMrRtTZDX"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "# Define unique path for current experiment\n",
        "PATH = \"/path/to/task1/outputs/{}/\".format((datetime.now() + timedelta(hours=2)).strftime(\"%d-%m-%Y-%H-%M\"))\n",
        "print(\"Current working dir:\", PATH)\n",
        "\n",
        "# Create directory\n",
        "os.mkdir(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_7xlRP57m2I"
      },
      "source": [
        "# Models' definition\n",
        "\n",
        "In this section, the different transformers that will be evaluated are gathered. For this purpose, the implementation mainly relies in the ``simpletransformers`` Python library, which allows to train and test transformers within few steps.\n",
        "\n",
        "For further information: https://simpletransformers.ai/\n",
        "\n",
        "**IMPORTANT NOTE:** although this is a binary classification task, it will be treated as a regression where a value between 0 and 1 must be predicted for each instance. Later, these predictions will be turned into binary values by the corresponding ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQQNWqu0a7Yt"
      },
      "outputs": [],
      "source": [
        "# Define transformers' initialization dictionary \n",
        "models = {\n",
        "    \"mbert-cased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"bert-base-multilingual-cased\"\n",
        "    },\n",
        "    \"mbert-uncased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"bert-base-multilingual-uncased\"\n",
        "    },\n",
        "    \"roberta\": {\n",
        "        \"model_type\": \"roberta\",\n",
        "        \"model_name\": \"roberta-base\"\n",
        "    },\n",
        "    \"beto-cased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"dccuchile/bert-base-spanish-wwm-cased\"\n",
        "    },\n",
        "    \"beto-uncased\": {\n",
        "        \"model_type\": \"bert\",\n",
        "        \"model_name\": \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "    },\n",
        "    \"distilbert-multi\": {\n",
        "        \"model_type\": \"distilbert\",\n",
        "        \"model_name\": \"distilbert-base-multilingual-cased\"\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqJi0pN8scJ8"
      },
      "outputs": [],
      "source": [
        "# Import pre-trained simpletransformers models for classification\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "# Define the number of labels for this task (a unique binary label)\n",
        "num_labels = 1\n",
        "\n",
        "# Define a dictionary where each key matches its corresponding transformer\n",
        "# All transformers share the same classification arguments\n",
        "for model, fields in models.items():    \n",
        "\n",
        "  # Define models' classification arguments\n",
        "  model_args = ClassificationArgs(\n",
        "      overwrite_output_dir= True,\n",
        "      regression=True,\n",
        "      eval_batch_size=8,\n",
        "      num_train_epochs=5,\n",
        "      learning_rate = 4e-05,\n",
        "      optimizer=\"AdamW\",\n",
        "      manual_seed=SEED,\n",
        "      use_early_stopping=True,\n",
        "      save_model_every_epoch=False\n",
        "  )\n",
        "\n",
        "  model_args.output_dir = os.path.join(PATH, model)\n",
        "  # os.mkdir(model_args.output_dir)\n",
        "  models[model] = ClassificationModel(fields[\"model_type\"], fields[\"model_name\"],\n",
        "                                      args=model_args, num_labels=num_labels, use_cuda=use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqfNi4v2-wqv"
      },
      "source": [
        "# Training\n",
        "\n",
        "Each of the aforementioned models is trained separatedly with the entire training set.\n",
        "\n",
        "This training is directly performed in the previously defined dictionary for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K5M7WQi2DIU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Define RMSE function\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return sqrt(mean_squared_error(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf82AZdd_F8k"
      },
      "outputs": [],
      "source": [
        "# Train all models with training set instances\n",
        "for model_name, model in models.items():\n",
        "  model.train_model(train, loss_fct=root_mean_squared_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff815Ug5igF3"
      },
      "source": [
        "# Ensembles' definition\n",
        "\n",
        "The ensembles of transformers that can be defined with the previously trained models are created.\n",
        "\n",
        "A dictionary is create for convenience, univocally identifying each ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q44p52krifIb"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from itertools import combinations\n",
        "\n",
        "# Define a list containing the lists of models of each ensemble\n",
        "models_names = list(models.keys())\n",
        "ensembles_list = list()\n",
        "\n",
        "for i in range(1, len(models_names) + 1):\n",
        "    ensembles_list += list(combinations(models_names, i))\n",
        "ensembles_list = [list(ensemble) for ensemble in ensembles_list]\n",
        "\n",
        "# Define a dictionary with the ensembles\n",
        "ensembles = {}\n",
        "for i in range(len(ensembles_list)):\n",
        "  ensembles[\"ensemble{:02d}\".format(i)] = {}\n",
        "  ensembles[\"ensemble{:02d}\".format(i)][\"models\"] = ensembles_list[i]\n",
        "  ensembles[\"ensemble{:02d}\".format(i)][\"metrics\"] = {}\n",
        "ensembles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phs1siYG7m2J"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Firstly, each transformer is individually evaluated using the validation split. Subsequently, the main evaluation metrics (accuracy, F1-score, precision and recall) are stored.\n",
        "\n",
        "Secondly, the predictions of each ensemble for the validation set instances are derived. After calculating their metrics, it is possible to determine which ensemble obtained the best F1-Score. This will be the final ensemble used for the test dataset.\n",
        "\n",
        "Regarding the ensembles' predictions, these are obtained through a hard voting system: after computing the output that each of the ensemble's models produces for a given instance, the most-voted class turns out to be the ensemble result.\n",
        "\n",
        "The voting system can be non-weighted or weighted. In the latter, the prediction of each individual transformer is weighted according to their normalized F1-score, thus providing a greater importance to the best model without disregarding the outputs of the other transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btUPwIXBAzcT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import time\n",
        "\n",
        "# Function which computes the evaluation metrics given two lists of true and\n",
        "# predicted labels\n",
        "def compute_metrics(y_true, y_pred):\n",
        "  precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  return {\n",
        "      'accuracy': round(acc, 5),\n",
        "      'f1': round(f1, 5),\n",
        "      'precision': round(precision, 5),\n",
        "      'recall': round(recall, 5)\n",
        "  }\n",
        "\n",
        "# Transformers' evaluation under the validation set\n",
        "model_evaluation = {}\n",
        "for model_name in models:\n",
        "  model_evaluation[model_name] = {}\n",
        "  # Storing the prediction outputs\n",
        "  result, model_outputs, wrong_predictions = models[model_name].eval_model(val, metric=root_mean_squared_error)\n",
        "  model_evaluation[model_name][\"result\"] = result                                                           # Result\n",
        "  model_evaluation[model_name][\"val_model_outputs\"] = model_outputs                                         # Raw model ouputs \n",
        "  model_evaluation[model_name][\"val_predictions\"] = [0 if output < 0.5 else 1 for output in model_outputs]  # Class prediction\n",
        "  model_evaluation[model_name][\"val_wrong_predictions\"] = wrong_predictions                                 # Wrongly-predicted instances\n",
        "  \n",
        "  # Storing the metrics\n",
        "  model_evaluation[model_name][\"metrics\"] = compute_metrics(val.get(\"label\"), model_evaluation[model_name].get(\"val_predictions\"))\n",
        "  print(f\"{model_name}\\t\", model_evaluation[model_name].get(\"metrics\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr7NHxjYo5xV"
      },
      "source": [
        "The ``vote`` function determines the ensembler prediction based on the outcomes of its transformers. Its arguments are:\n",
        "1.   ``predictions``: list of transformers' (raw) outputs\n",
        "2.   ``weighted``: bool that determines if a weighted voting system must be used\n",
        "3.   ``weights``: list of weights (normalized F1-scores) \n",
        "\n",
        "The ``predict_ensemble``function calculates the predictions of each ensemble for a given dataset split (``dataset_name``, ``dataset``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "echFpwerAO_n"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Function which determines the ensembler prediction based on its\n",
        "# transformers' predictions. A weighted voting system may be used\n",
        "def vote(predictions, weighted=False, weights=None):\n",
        "  voting = sum(predictions * weights) if weighted else sum(predictions)/len(predictions)\n",
        "  return 0 if voting < 0.5 else 1\n",
        "\n",
        "ensemble_evaluation = {}\n",
        "\n",
        "# Function to predict the label of the instances in a dataset split (validation\n",
        "# (\"val\") or test (\"test\")) for each ensemble\n",
        "def predict_ensemble(ensemble_name, dataset_name, dataset, weighted=False):\n",
        "  ensemble_evaluation[ensemble_name][f\"{dataset_name}_predictions\"] = list()\n",
        "  # Traverse each dataset instance\n",
        "  for i in range(len(dataset.index)):\n",
        "    predictions = list()\n",
        "    ensemble_models = ensembles[ensemble_name].get(\"models\")\n",
        "    # Get the raw output of each model in the ensemble for the instance at hand\n",
        "    for model_name in ensemble_models:\n",
        "      curr_model_outputs = model_evaluation[model_name].get(f\"{dataset_name}_model_outputs\")\n",
        "      predictions.append(curr_model_outputs[i])\n",
        "    \n",
        "    # Define the list of weights if a weighted voting system must be used\n",
        "    weights = list()\n",
        "    if weighted:\n",
        "      # The weights' list is obtained by normalizing the F1-scores of the models\n",
        "      # in the ensemble\n",
        "      f1_scores_list = [model_evaluation[model_name][\"metrics\"].get(\"f1\")\n",
        "                        for model_name in ensembles[ensemble_name].get(\"models\")]\n",
        "      weights = normalize([f1_scores_list], norm=\"l1\")[0]\n",
        "\n",
        "    # Append the predicted label to the predictions of the ensemble\n",
        "    ensemble_pred = vote(predictions, weighted, weights)\n",
        "    ensemble_evaluation[ensemble_name][f\"{dataset_name}_predictions\"].append(ensemble_pred)\n",
        "\n",
        "# Ensembles' evaluation under the validation set\n",
        "for ensemble_name in ensembles:\n",
        "  ensemble_evaluation[ensemble_name] = {}\n",
        "  ensemble_evaluation[ensemble_name][\"val_predictions\"] = list()\n",
        "  predict_ensemble(ensemble_name, \"val\", val, weighted=WEIGHTED)\n",
        "  ensembles[ensemble_name][\"metrics\"] = compute_metrics(val.get(\"label\"), ensemble_evaluation[ensemble_name].get(\"val_predictions\"))\n",
        "  print(f\"{ensemble_name}\\t\", ensembles[ensemble_name].get(\"metrics\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExO93OK0rCnQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save ensembles to JSON file\n",
        "with open(os.path.join(PATH, 'ensembles.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(ensembles, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ2VBWpGt1Ob"
      },
      "source": [
        "# Selecting the best ensemble\n",
        "\n",
        "Once the predicted labels for each validation instance are calculated for each ensemble, their metrics can be computed. Given that it is a binary classification task, the best ensemble will be that with a maximum F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOuzbgJax6oZ"
      },
      "outputs": [],
      "source": [
        "# Defining a dictionary with the F1-score of each ensemble\n",
        "f1_scores = {ensemble_name: ensembles[ensemble_name][\"metrics\"].get(\"f1\") for ensemble_name in ensemble_evaluation}\n",
        "# Selecting the best ensemble\n",
        "best_ensemble_name = max(f1_scores, key=f1_scores.get)\n",
        "best_ensemble = {\"name\": best_ensemble_name,\n",
        "                 \"models\": ensembles[best_ensemble_name].get(\"models\"),\n",
        "                 \"metrics\": ensembles[best_ensemble_name].get(\"metrics\")\n",
        "                 }\n",
        "\n",
        "best_ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyNjhkTUF3eU"
      },
      "source": [
        "# Predictions on test set\n",
        "\n",
        "Finally, the ensemble which obtained a higher F1-score can be used to predict the label of each test instance.\n",
        "\n",
        "Further, these results will be used to portray some evaluation plots, including the Confusion Matrix on the positive class (\"humour\") and the ROC curve. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OPMaerZF5IA"
      },
      "outputs": [],
      "source": [
        "# Predicting the label of the test set's instances with each individual transformer\n",
        "for model_name in models:\n",
        "  model_predictions, model_raw_outputs = models.get(model_name).predict(test[\"text\"].tolist())\n",
        "  model_evaluation[model_name][\"test_model_outputs\"] = model_raw_outputs\n",
        "  model_evaluation[model_name][\"test_predictions\"] = [0 if output < 0.5 else 1 for output in model_raw_outputs]\n",
        "\n",
        "# Calculating the test predictions of the best ensemble\n",
        "predict_ensemble(best_ensemble.get(\"name\"), \"test\", test, weighted=WEIGHTED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGP56svZssow"
      },
      "outputs": [],
      "source": [
        "# Dump individual transformers' results\n",
        "for model_name, evaluation in model_evaluation.items():\n",
        "\n",
        "  curr_model = model_evaluation.get(model_name)\n",
        "\n",
        "  # Converting ndarrays to lists\n",
        "  curr_model[\"val_model_outputs\"] = list(curr_model.get(\"val_model_outputs\"))\n",
        "  curr_model[\"test_model_outputs\"] = list(curr_model.get(\"test_model_outputs\"))\n",
        "  curr_model[\"val_predictions\"] = list(curr_model.get(\"val_predictions\"))\n",
        "  curr_model[\"test_predictions\"] = list(curr_model.get(\"test_predictions\"))\n",
        "  \n",
        "  # Adapting validation wrong predictions (if any)\n",
        "  if curr_model.get(\"val_wrong_predictions\"):\n",
        "    curr_model[\"val_wrong_predictions_list\"] = curr_model.get(\"val_wrong_predictions\")\n",
        "    curr_model[\"val_wrong_predictions\"] = {}\n",
        "    for pred in curr_model.get(\"val_wrong_predictions_list\"):\n",
        "      curr_model[\"val_wrong_predictions\"][pred.guid] = {\n",
        "          \"text_a\": pred.text_a,\n",
        "          \"text_b\": pred.text_b,\n",
        "          \"label\": pred.label\n",
        "      }\n",
        "    del curr_model[\"val_wrong_predictions_list\"]\n",
        "\n",
        "  with open(os.path.join(PATH, f'{model_name}/model-evaluation.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(curr_model, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cpU0o6l7ClV"
      },
      "outputs": [],
      "source": [
        "# Complete fields of best ensemble dictionary\n",
        "best_ensemble[\"val_predictions\"] = ensemble_evaluation[best_ensemble.get(\"name\")].get(\"val_predictions\")\n",
        "best_ensemble[\"test_predictions\"] = ensemble_evaluation[best_ensemble.get(\"name\")].get(\"test_predictions\")\n",
        "\n",
        "# Save best ensemble to JSON file\n",
        "with open(os.path.join(PATH, 'best-ensemble.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(best_ensemble, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOQ10-8OGE8x"
      },
      "outputs": [],
      "source": [
        "# Creating a new column of predicted labels in the test dataframe\n",
        "test[\"predicted_label\"] = ensemble_evaluation[best_ensemble.get(\"name\")].get(\"test_predictions\")\n",
        "test.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mObj5Iy7DV6"
      },
      "outputs": [],
      "source": [
        "# Dump test predictions\n",
        "test.to_csv(os.path.join(PATH, \"test-predictions.csv\"), index=False)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwdzIe6Pv3Ke"
      },
      "source": [
        "## Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQLzXbt8plBt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Classification report\n",
        "labels = [\"NO-Humor\", \"YES-Humor\"]\n",
        "cr_str = classification_report(y_true=test[\"label\"].tolist(), y_pred=test[\"predicted_label\"].tolist(), target_names=labels)\n",
        "cr = classification_report(y_true=test[\"label\"].tolist(), y_pred=test[\"predicted_label\"].tolist(), target_names=labels, output_dict=True)\n",
        "\n",
        "fig_cr = plt.figure(figsize = (10.8, 10.8))\n",
        "sns.heatmap(pd.DataFrame(cr).iloc[:-1, :].T, annot = True, fmt = \".2f\", cbar_kws = {\"shrink\" : 0.5}, annot_kws = {\"size\": 15})\n",
        "plt.xlabel(\"Evaluated Metrics\", fontsize = 15)\n",
        "plt.ylabel(\"Classes & Metrics\", fontsize = 15)\n",
        "plt.title(\"Evaluation Metrics\", fontsize = 20)\n",
        "\n",
        "# Save CR\n",
        "fig_cr.savefig(os.path.join(PATH, \"CLASSIFICATION_REPORT.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nLSA1_wrpng"
      },
      "outputs": [],
      "source": [
        "# Dump classification report\n",
        "with open(os.path.join(PATH, \"classification-report.txt\"), \"w\") as f:\n",
        "  f.write(cr_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVeIDMGBv4-v"
      },
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgyj9Pa_RBK9"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "LABELS = [\"NO-Humor\", \"YES-Humor\"]\n",
        "fig_cm = plt.figure(figsize = (10.8, 10.8))\n",
        "cm = confusion_matrix(test.get(\"label\"), test.get(\"predicted_label\"), normalize = \"true\")\n",
        "sns.heatmap(cm, vmin = 0, vmax = 1, square = True, annot = True, fmt = \".2f\", cbar_kws = {\"shrink\" : 0.5}, xticklabels = LABELS, yticklabels = LABELS, annot_kws = {\"size\": 15})\n",
        "plt.xlabel(\"Predicted Values\", fontsize = 15)\n",
        "plt.ylabel(\"True Values\", fontsize = 15)\n",
        "plt.title(\"Confusion Matrix\", fontsize = 20)\n",
        "\n",
        "# Save CM\n",
        "fig_cm.savefig(os.path.join(PATH, \"CONFUSION MATRIX.png\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhVghIyzv-U6"
      },
      "source": [
        "## ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTOFdCbAFhln"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig_roc = plt.figure(figsize = (10.8, 10.8))\n",
        "fpr, tpr, _ = roc_curve(test[\"label\"], test[\"predicted_label\"])\n",
        "plt.plot(fpr, tpr, color = \"darkorange\", label = \"{} (AUC = {:0.2f})\".format(best_ensemble.get(\"name\"), auc(fpr, tpr)))\n",
        "plt.plot([0, 1], [0, 1], \"k--\", color = \"darkblue\", linestyle = \"--\", label = \"Random Classifier (AUC = 0.5)\") # AUC: Area Under Curve\n",
        "plt.axis(\"square\")\n",
        "plt.xlabel(\"False Positive Rate (FPR)\", fontsize = 15)\n",
        "plt.ylabel(\"True Positive Rate (TPR)\", fontsize = 15)\n",
        "plt.title(\"ROC curve\", fontsize = 20)\n",
        "plt.tick_params(axis = \"y\",direction = \"in\")\n",
        "plt.tick_params(axis = \"x\",direction = \"in\")\n",
        "plt.legend()\n",
        "\n",
        "# Save ROC curve\n",
        "fig_roc.savefig(os.path.join(PATH, \"ROC.png\"))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
